{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先使用bs4爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-15 10:00 遭通報之境外帳戶(更新至1120914)\n",
      "2023-08-15 14:45 全家便利商店 推出 Fun心取爭議包裹退貨申請平臺\n",
      "2023-08-10 12:00 出國前訂購網卡 接到訂單錯誤來電??\n",
      "2023-08-09 17:43 小心釣魚簡訊，收到簡訊附網址請先查詢！\n",
      "2023-08-07 09:25 【網站維護公告】112/08/08 11:30-13:30\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 這裡假設 html_content 是你從該網頁獲得的HTML內容\n",
    "html_content = \"\"\"\n",
    "<li _ngcontent-yby-c4=\"\"><p _ngcontent-yby-c4=\"\" class=\"date\">2023-09-15 10:00</p><span _ngcontent-yby-c4=\"\" style=\"cursor: pointer\">遭通報之境外帳戶(更新至1120914)</span></li>\n",
    "\n",
    "<li _ngcontent-yby-c4=\"\"><p _ngcontent-yby-c4=\"\" class=\"date\">2023-08-15 14:45</p><span _ngcontent-yby-c4=\"\" style=\"cursor: pointer\">全家便利商店 推出 Fun心取爭議包裹退貨申請平臺</span></li>\n",
    "\n",
    "<li _ngcontent-yby-c4=\"\"><p _ngcontent-yby-c4=\"\" class=\"date\">2023-08-10 12:00</p><span _ngcontent-yby-c4=\"\" style=\"cursor: pointer\">出國前訂購網卡 接到訂單錯誤來電??</span></li>\n",
    "\n",
    "<li _ngcontent-yby-c4=\"\"><p _ngcontent-yby-c4=\"\" class=\"date\">2023-08-09 17:43</p><span _ngcontent-yby-c4=\"\" style=\"cursor: pointer\">小心釣魚簡訊，收到簡訊附網址請先查詢！</span></li>\n",
    "\n",
    "<li _ngcontent-yby-c4=\"\"><p _ngcontent-yby-c4=\"\" class=\"date\">2023-08-07 09:25</p><span _ngcontent-yby-c4=\"\" style=\"cursor: pointer\">【網站維護公告】112/08/08 11:30-13:30</span></li>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'lxml')  # 使用 lxml 解析器\n",
    "\n",
    "# 提取所有的新聞日期和標題\n",
    "news_data = []\n",
    "for li in soup.find_all('li', {'_ngcontent-yby-c4': ''}):\n",
    "    date = li.find('p', class_='date').text\n",
    "    title = li.find('span').text\n",
    "    news_data.append((date, title))\n",
    "\n",
    "# 顯示結果\n",
    "for date, title in news_data:\n",
    "    print(date, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用selenium爬第一頁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-15 10:00 遭通報之境外帳戶(更新至1120914)\n",
      "2023-08-15 14:45 全家便利商店 推出 Fun心取爭議包裹退貨申請平臺\n",
      "2023-08-10 12:00 出國前訂購網卡 接到訂單錯誤來電??\n",
      "2023-08-09 17:43 小心釣魚簡訊，收到簡訊附網址請先查詢！\n",
      "2023-08-07 09:25 【網站維護公告】112/08/08 11:30-13:30\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "URL = \"https://165.npa.gov.tw/#/articles/1\"\n",
    "\n",
    "# 啟動瀏覽器並打開網頁\n",
    "service = Service('chromedriver.exe')\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service,options=options)\n",
    "driver.get(URL)\n",
    "\n",
    "# 等待網頁加載完成\n",
    "wait = WebDriverWait(driver, 20)  # 增加到 20 秒\n",
    "\n",
    "# 使用明確的方法查找元素\n",
    "try:\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li > p.date')))\n",
    "\n",
    "    # 獲取所有相匹配的元素\n",
    "    news_elements = driver.find_elements(By.CSS_SELECTOR, '.articleList > li')\n",
    "\n",
    "    if not news_elements:\n",
    "        print(\"未找到任何匹配的新聞元素。\")\n",
    "    else:\n",
    "        for news in news_elements:\n",
    "            date = news.find_element(By.CSS_SELECTOR, 'p.date').text\n",
    "            title = news.find_element(By.CSS_SELECTOR, 'span').text\n",
    "            print(f\"{date} {title}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"發生錯誤: {e}\")\n",
    "\n",
    "finally:\n",
    "    # 關閉瀏覽器\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用selenium爬第一頁+內文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-15 10:00 遭通報之境外帳戶(更新至1120914):\n",
      "NA\n",
      "\n",
      "2023-08-15 14:45 全家便利商店 推出 Fun心取爭議包裹退貨申請平臺:\n",
      "什麼是一頁式廣告跨境包裹詐騙? 詐騙集團利用社群平臺創立假粉絲專頁投放廣告，以低於市場行情之價格促銷各類商品，吸引民眾目光，且此類廣告以「貨到付款」方式取信民眾，等到消費者收貨付款後卻發現商品與介紹不符，品質低劣。 那...付出去的錢怎麼拿回來?! 全家便利商店於今年7月初推出爭議包裹退貨申請平臺服務，民眾線上申請後，帶著原包裹到任一家全家超商印單退貨。廠商收到退貨 後，進行退貨包裹內容確認，確認後將於10日內完成退款，期降低民眾自行聯繫之困擾。 申請平臺及詳細說明 : https://returns.com.tw/web/index.php 全家平臺可接受處理的『詐騙包裹』定義如下： 1、消費者並未下單，卻收到到貨簡訊並至全家店舖完成繳費取貨。 2、消費者取貨後發現為空包裹，並提供相關資料佐證。 3、其他問題將視為『消費糾紛』，將依申訴內容受理，但不保障退款，該平台會協助通知廠商處理。 最重要的，還是認明一頁式廣告特徵，才可以免除後續困擾!\n",
      "\n",
      "2023-08-10 12:00 出國前訂購網卡 接到訂單錯誤來電??:\n",
      "疫情解封後，國人開始有出國旅遊的規劃 近期有民眾通報表示接獲詐騙集團假冒「JOYTEL卓一電訊」名義，稱之前訂購網卡、SIM卡，因訂單操作錯誤，需操作ATM或網路銀行以解除分期付款。 請注意：如接獲來電顯示開頭帶「+」號、「+1」、「+886」等號碼，電話中如聽到關鍵字「解除重複扣款」、「誤設會員等級」、「登記批發商」等關鍵字，請立即掛斷，並將相關資訊先向電商業者反映，如有被害請通報165反詐騙諮詢專線。\n",
      "\n",
      "2023-08-09 17:43 小心釣魚簡訊，收到簡訊附網址請先查詢！:\n",
      "若您收到簡訊，內容是水費、電費、汽燃費逾期，要求您點擊短網址連結進行繳費??? 請小心!!! 不要點擊連結，小心個資遭盜 或 信用卡被盜刷。 收到要求繳費之簡訊，請先上各單位官網查詢真偽，避免遭詐。 臺灣自來水公司 https://www.water.gov.tw/ 臺灣電力股份有限公司 https://www.taipower.com.tw/tc/index.aspx 監理服務網 https://www.mvdis.gov.tw/m3-emv-fee/fee/fuelFee#gsc.tab=0\n",
      "\n",
      "2023-08-07 09:25 【網站維護公告】112/08/08 11:30-13:30:\n",
      "165全民防騙網將於112/08/08 11:30-13:30進行維護，屆時查詢及線上報案、檢舉功能將暫停服務，如有疑問請撥打165反詐騙諮詢專線，造成您的諸多不便，敬請見諒。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "URL = \"https://165.npa.gov.tw/#/articles/1\"\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# 啟動瀏覽器並打開網頁\n",
    "service = Service('chromedriver.exe')\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service,options=options)\n",
    "driver.get(URL)\n",
    "\n",
    "# 等待網頁加載完成\n",
    "wait = WebDriverWait(driver, 10)  \n",
    "df = pd.DataFrame(columns=[\"日期\", \"新聞標題\", \"內文\"])\n",
    "\n",
    "# 使用明確的方法查找元素\n",
    "try:\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li > p.date')))\n",
    "    \n",
    "    news_count = len(driver.find_elements(By.CSS_SELECTOR, '.articleList > li'))\n",
    "    \n",
    "    for index in range(news_count):\n",
    "        # 每次迭代都重新找到新聞元素，避免 stale element problems\n",
    "        news_elements = driver.find_elements(By.CSS_SELECTOR, '.articleList > li')\n",
    "        news = news_elements[index]\n",
    "        \n",
    "        date = news.find_element(By.CSS_SELECTOR, 'p.date').text\n",
    "        title_element = news.find_element(By.CSS_SELECTOR, 'span')\n",
    "        title = title_element.text\n",
    "\n",
    "        # 模擬點擊事件\n",
    "        title_element.click()\n",
    "\n",
    "        try:\n",
    "            # 等待頁面中的文字內容加載完成，最多等待10秒\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div p span')))\n",
    "\n",
    "            # 提取新聞內文\n",
    "            content_elements = driver.find_elements(By.CSS_SELECTOR, 'div p > span')\n",
    "            content = ' '.join([elem.text for elem in content_elements if not elem.find_elements(By.CSS_SELECTOR, \"img\")])\n",
    "            \n",
    "        except TimeoutException:\n",
    "            # 如果超過10秒還沒找到元素，設定內容為 \"NA\"\n",
    "            content = \"NA\"\n",
    "\n",
    "        print(f\"{date} {title}:\\n{content}\\n\")\n",
    "        \n",
    "        # 將新聞資料添加到DataFrame中\n",
    "        df.loc[len(df)] = [date, title, content]\n",
    "\n",
    "        # 返回到主頁面\n",
    "        driver.back()\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li > p.date')))  # 確保新聞元素再次出現\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"發生錯誤: {e}\")\n",
    "\n",
    "# 保存DataFrame到Excel文件\n",
    "df.to_excel(f'{current_date}_165新聞快訊爬蟲.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "# 關閉瀏覽器\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抓好幾頁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "遭通報之境外帳戶(更新至1120914)\n",
      "全家便利商店 推出 Fun心取爭議包裹退貨申請平臺\n",
      "出國前訂購網卡 接到訂單錯誤來電??\n",
      "小心釣魚簡訊，收到簡訊附網址請先查詢！\n",
      "【網站維護公告】112/08/08 11:30-13:30\n",
      "【全民齊反詐】1000個正忠排骨飯等你來抽獎！\n",
      "常見詐欺六大手法\n",
      "小心! 【社群影音平臺】也暗藏遭詐騙風險!\n",
      "我們要的就是你 !!!\n",
      "我國網路詐欺被害調查與防制研究\n",
      "見面再談，匯錢? 免談!\n",
      "防制詐騙簡訊民眾有感 半年減少近9成\n",
      "遇到社群平臺一頁式廣告詐騙? 申請退款流程說明!\n",
      "演唱會沒看到，還被騙錢!!\n",
      "你被詐騙話術了嗎?\n",
      "每人普發現金6000元方案不會用簡訊通知 請勿點擊不明連結\n",
      "假冒銀行信用貸款簡訊\n",
      "虛擬遊戲詐騙手法大公開\n",
      "個資防護，從密碼開始\n",
      "查證新管道!165全民防騙網可以線上查詢詐騙資訊囉\n",
      "一點就詐!小心釣魚簡訊讓手機中毒!\n",
      "網路求職、借貸 2 No! No!\n",
      "111.11.11購物節，我們搶先推出!!!!\n",
      "網路求職 一堆騙子! 小心社群平臺求職詐騙\n",
      "沒認清投資詐騙誘惑，小心誤上賊船！！！！！\n",
      "一分錢一分貨，小心奧梨子假蘋果\n",
      "假紓困、真詐財，假冒官網網站激增!\n",
      "幫好友輔助認證? LINE帳號會被盜!\n",
      "安卓用戶照過來 165教您 開啟垃圾訊息阻擋功能阻擋詐騙簡訊\n",
      "iPhone用戶照過來 165教您 阻絕透過iMessage 發送的詐騙簡訊\n",
      "112/9/6-112/9/12民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/9/4~112/9/10詐騙來電排名\n",
      "112/8/29-112/9/5民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/8/28-112/9/3詐騙來電排名\n",
      "112/8/22-112/8/28民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/8/21-112/8/27詐騙來電排名\n",
      "112/8/15-112/8/21民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/8/14-112/8/20詐騙來電排名\n",
      "112/8/8-112/8/14民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/8/7-112/8/13詐騙來電排名\n",
      "112/8/1-112/8/7民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/7/31-112/8/6詐騙來電排名\n",
      "112/7/25-112/7/31民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/7/24-112/7/30詐騙來電排名\n",
      "112/7/18-112/7/24民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/7/17-112/7/23詐騙來電排名\n",
      "投資詐騙新話術~見警語速撥165查證\n",
      "112/7/11-112/7/17民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/7/10-112/7/16詐騙來電排名\n",
      "112/7/4-112/7/10民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/7/3-112/7/9詐騙來電排名\n",
      "112/6/26-112/7/3民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/6/26-112/7/2詐騙來電排名\n",
      "112/6/19-112/6/25民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/6/19-112/6/25詐騙來電排名\n",
      "112/6/12-112/6/18民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/6/12-112/6/18詐騙來電排名\n",
      "112/6/5-112/6/11民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/6/5-112/6/11詐騙來電排名\n",
      "112/5/29-112/6/4民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/5/29-112/6/4詐騙來電排名\n",
      "112/5/22-112/5/28民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/5/22-112/5/28詐騙來電排名\n",
      "112/5/15-112/5/21民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/5/15-112/5/21詐騙來電排名\n",
      "112/5/8-112/5/14民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/5/8-112/5/14詐騙來電排名\n",
      "遭通報之境外帳戶1100101-1231\n",
      "遭通報之境外帳戶1090101-1231\n",
      "112/5/1-112/5/7民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/5/1-112/5/7詐騙來電排名\n",
      "釣魚連結騙不停 填輸資料前請認證官方網頁\n",
      "112/4/25-112/4/30民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n",
      "112/4/24-112/4/30詐騙來電排名\n",
      "112/4/18-112/4/24民眾通報假投資(博弈)詐騙網站 【網友不會幫你賺錢、請勿聽信網友投資】\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "URL = \"https://165.npa.gov.tw/#/articles/1\"\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "service = Service('chromedriver.exe')\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.get(URL)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "df = pd.DataFrame(columns=[\"日期\", \"新聞標題\", \"內文\"])\n",
    "\n",
    "def scrape_page():\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li > p.date')))\n",
    "        news_count = len(driver.find_elements(By.CSS_SELECTOR, '.articleList > li'))\n",
    "        for index in range(news_count):\n",
    "            news_elements = driver.find_elements(By.CSS_SELECTOR, '.articleList > li')\n",
    "            news = news_elements[index]\n",
    "            date = news.find_element(By.CSS_SELECTOR, 'p.date').text\n",
    "            title_element = news.find_element(By.CSS_SELECTOR, 'span')\n",
    "            title = title_element.text\n",
    "            title_element.click()\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div p span')))\n",
    "                content_elements = driver.find_elements(By.CSS_SELECTOR, 'div p > span')\n",
    "                content = ' '.join([elem.text for elem in content_elements if not elem.find_elements(By.CSS_SELECTOR, \"img\")])\n",
    "            except TimeoutException:\n",
    "                content = \"N/A\"\n",
    "            print(f\"{title}\")\n",
    "            df.loc[len(df)] = [date, title, content]\n",
    "            driver.back()\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li > p.date')))\n",
    "    except Exception as e:\n",
    "        print(f\"發生錯誤: {e}\")\n",
    "\n",
    "# 爬取首頁的新聞\n",
    "scrape_page()\n",
    "\n",
    "#設定要爬幾篇\n",
    "page = 15\n",
    "\n",
    "# 爬取第幾頁的新聞\n",
    "for i in range(2, page+1):  \n",
    "    try:\n",
    "        page_button = driver.find_element(By.XPATH, f\"//li/a[text()='{i}']\")\n",
    "        page_button.click()\n",
    "        scrape_page()\n",
    "    except Exception as e:\n",
    "        print(f\"Error navigating to page {i}: {e}\")\n",
    "    \n",
    "df.to_excel(f'{current_date}_{page}頁_165新聞快訊.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
